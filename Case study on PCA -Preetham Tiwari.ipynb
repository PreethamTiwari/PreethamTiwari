{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab3c55c",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "You work as a data scientist in a flower research company. The company has a sample dataset of prelabeled\n",
    "data on iris dataset with features like 'sepal-length', 'sepal-width', 'petal-length', 'petal-width'\n",
    "and 'Class'. They plan to extend this dataset and train a RandomForestClassifier on it. But they expect\n",
    "the dataset to grow quite large i.e. millions of rows and are worried that a million rows and 4 features is\n",
    "going to be too big for them to be able to train their classifier. They wish to reduce the number of\n",
    "features or dimensions without a sharp decrease in accuracy of the classifier.\n",
    "\n",
    "You have been asked to:\n",
    "1. Read the sample dataset given to you.\n",
    "2. Use PCA to figure out the number of most important principle features.\n",
    "3. Reduce the number of features using PCA\n",
    "4. Train and test the RandomForestClassifier algorithm to check if reducing the number of\n",
    "dimensions is causing the model to perform poorly.\n",
    "5. Figure out the most optimal number of components that produce good quality results i.e. they\n",
    "do not cause a sharp decrease in prediction accuracy.\n",
    "6. Do this for all possible number of principle components and find out the smallest number of\n",
    "components that our dataset can be reduced to with good prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af78e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b01b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "column_names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
    "dataset = pd.read_csv('iris.csv', names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c2afc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal-length  sepal-width  petal-length  petal-width        Class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b35e7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the date into train and test\n",
    "X = dataset.drop('Class', 1)\n",
    "Y = dataset['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e707452c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92597835, 0.0536899 , 0.01568407, 0.00464768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f4e090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusuion Matrix with 1 Principle Components: \n",
      "[[11  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  2  4]]\n",
      "Accuracy Score with 1 Principle Components: 0.9\n",
      "\n",
      "Confusuion Matrix with 2 Principle Components: \n",
      "[[11  0  0]\n",
      " [ 0  8  5]\n",
      " [ 0  1  5]]\n",
      "Accuracy Score with 2 Principle Components: 0.8\n",
      "\n",
      "Confusuion Matrix with 3 Principle Components: \n",
      "[[11  0  0]\n",
      " [ 0  5  8]\n",
      " [ 0  1  5]]\n",
      "Accuracy Score with 3 Principle Components: 0.7\n",
      "\n",
      "Confusuion Matrix with 4 Principle Components: \n",
      "[[11  0  0]\n",
      " [ 0 11  2]\n",
      " [ 0  1  5]]\n",
      "Accuracy Score with 4 Principle Components: 0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, len(X.columns) + 1):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    pca = PCA(n_components=n)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    print(\"Confusuion Matrix with {0} Principle Components: \".format(n))\n",
    "    print(cm)\n",
    "    print(\"Accuracy Score with {0} Principle Components: \".format(n), end=\"\")\n",
    "    print(accuracy_score(Y_test, Y_pred), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf13be0",
   "metadata": {},
   "source": [
    "From the above result it can be seen that the confusion matric with 1 principle component produces the same result as the confusion matrix with 4 principle components .The smallest number of component to produce good accuracy is 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd616d45",
   "metadata": {},
   "source": [
    "# END OF ASSIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e483c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
